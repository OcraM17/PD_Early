{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('PATH/PPMI_Dataset/Subject_Characteristics/Screening___Demographics.csv')\n",
    "x[x['APPRDX']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('PATH/PPMI_Dataset/Subject_Characteristics/PPMI_Baseline_Data_02Jul2018.csv')\n",
    "PD_patlist=x[x['APPRDX']==1]['PATNO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months=['BL','V04','V06','V08','V10']\n",
    "files=[]\n",
    "for period in months:\n",
    "    \n",
    "    y=pd.read_csv('PATH/PPMI_Dataset/Motor_Assessments/MDS_UPDRS_Part_III.csv')\n",
    "    y=y[y['PATNO'].isin(PD_patlist)]\n",
    "    y=y[y['EVENT_ID']==period]\n",
    "    y=y[(y['PAG_NAME']=='NUPDRS3')]\n",
    "    \n",
    "    y=y.iloc[:,np.r_[2,11:44]]\n",
    "    y=y.sort_values(by=['PATNO']).reset_index()\n",
    "    files.append(y.drop(columns='index'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in files:\n",
    "    df['Total_UPDRS3']=df.loc[:,'NP3SPCH':'NP3RTCON'].sum(axis=1,skipna=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog=pd.DataFrame()\n",
    "prog['PATNO']=files[0]['PATNO']\n",
    "prog['BL_UPDRS3']=files[0]['Total_UPDRS3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog=prog.merge(files[1].loc[:,['PATNO','Total_UPDRS3']],how='left',on='PATNO')\n",
    "prog=prog.rename(columns={'Total_UPDRS3':'year1_UPDRS3'})\n",
    "\n",
    "prog=prog.merge(files[2].loc[:,['PATNO','Total_UPDRS3']],how='left',on='PATNO')\n",
    "prog=prog.rename(columns={'Total_UPDRS3':'year2_UPDRS3'})\n",
    "\n",
    "prog=prog.merge(files[3].loc[:,['PATNO','Total_UPDRS3']],how='left',on='PATNO')\n",
    "prog=prog.rename(columns={'Total_UPDRS3':'year3_UPDRS3'})\n",
    "\n",
    "prog=prog.merge(files[4].loc[:,['PATNO','Total_UPDRS3']],how='left',on='PATNO')\n",
    "prog=prog.rename(columns={'Total_UPDRS3':'year4_UPDRS3'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog['Diff_BL_1yr']=prog['year1_UPDRS3']-prog['BL_UPDRS3']\n",
    "prog['Diff_BL_2yr']=prog['year2_UPDRS3']-prog['BL_UPDRS3']\n",
    "prog['Diff_BL_3yr']=prog['year3_UPDRS3']-prog['BL_UPDRS3']\n",
    "prog['Diff_BL_4yr']=prog['year4_UPDRS3']-prog['BL_UPDRS3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog['Diff_BL_4yr'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "yr='4'\n",
    "sns.distplot(prog['Diff_BL_'+yr+'yr'],kde_kws={'shade':True,'linewidth':2})\n",
    "plt.savefig('Diff_BL_'+yr+'yr')\n",
    "\n",
    "\n",
    "x=prog.loc[:,['PATNO','Diff_BL_'+yr+'yr']].dropna().sort_values(by=['Diff_BL_'+yr+'yr'],ascending=False)\n",
    "group_early=x[x['Diff_BL_'+yr+'yr']>20]['PATNO']\n",
    "group_others=x[x['Diff_BL_'+yr+'yr']<15]['PATNO']\n",
    "print('20% are early progressor. Patients available: ',len(group_early))\n",
    "print('80% are other patient. Patients available: ',len(group_others))\n",
    "\n",
    "plt.hist(prog['Diff_BL_'+yr+'yr'],bins=np.arange(-40,81,5),color='b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BL_Feat=pd.DataFrame()\n",
    "BL_Feat['PATNO']=PD_patlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vital signs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Medical_History/Vital_Signs.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='BL']\n",
    "x=x.iloc[:,np.r_[2,6:9,10:16]]\n",
    "x['BMI']=x['WGTKG']/((x['HTCM']/100)*(x['HTCM']/100))\n",
    "BL_Feat=BL_Feat.merge(x.loc[:,['PATNO','BMI','SYSSUP','HRSUP']],on='PATNO',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Subject_Characteristics/PPMI_Baseline_Data_02Jul2018.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='BL']\n",
    "x=x.iloc[:,np.r_[1,4,6,7,14,15,16]]\n",
    "BL_Feat=BL_Feat.merge(x,on='PATNO',how='left')\n",
    "BL_Feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motor Symptoms\n",
    "## MDS-UPDRS I\n",
    "### aggregate scores: Cognitive, sleep, Autonomic nervous system and total updrs 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Motor_Assessments/MDS_UPDRS_Part_I.csv')\n",
    "y=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Motor_Assessments/MDS_UPDRS_Part_I__Patient_Questionnaire.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='BL']\n",
    "\n",
    "y=y[y['PATNO'].isin(PD_patlist)]\n",
    "y=y[y['EVENT_ID']=='BL']\n",
    "x=x.iloc[:,np.r_[2,7:13]]\n",
    "y=y.iloc[:,np.r_[2,7:14]]\n",
    "x=x.merge(y,how='outer',on='PATNO')\n",
    "x['Cognitive']=x.loc[:,['NP1COG','NP1HALL','NP1DPRS','NP1ANXS','NP1APAT','NP1DDS']].sum(axis=1,skipna=False)\n",
    "x['Sleep']=x.loc[:,['NP1SLPN','NP1SLPD']].sum(axis=1,skipna=False)\n",
    "x['Autonomic_Nervous_System']=x.loc[:,['NP1URIN','NP1CNST','NP1LTHD','NP1FATG']].sum(axis=1,skipna=False)\n",
    "x['Total_UPDRS1']=x.loc[:,'NP1COG':'NP1FATG'].sum(axis=1,skipna=False)\n",
    "\n",
    "BL_Feat=BL_Feat.merge(x.iloc[:,np.r_[0,14:18]],on='PATNO',how='left')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDS-UPDRS II\n",
    "### aggregate scores: Bulbar, common daily activity, bed, gait and total updrs 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Motor_Assessments/MDS_UPDRS_Part_II__Patient_Questionnaire.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='BL']\n",
    "x['Bulbar']=x.loc[:,['NP2SPCH','NP2SALV','NP2SWAL','NP2EAT']].sum(axis=1,skipna=False)\n",
    "x['Common_daily_act']= x.loc[:,['NP2DRES','NP2HYGN','NP2HWRT','NP2HOBB']].sum(axis=1,skipna=False)\n",
    "x['Bed']=x.loc[:,['NP2TURN','NP2RISE']].sum(axis=1,skipna=False)\n",
    "x['Gait']=x.loc[:,['NP2WALK','NP2FREZ']].sum(axis=1,skipna=False)\n",
    "x['Total_UPDRS2']=x.loc[:,'NP2SPCH':'NP2FREZ'].sum(axis=1,skipna=False)\n",
    "x=x.loc[:,['PATNO','Bulbar','Common_daily_act','Bed','Gait','Total_UPDRS2']]\n",
    "BL_Feat=BL_Feat.merge(x,on='PATNO',how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDS-UPDRS III\n",
    "### aggregate scores: Axial subscore 1, axial subscore 2, limb rigidity, limb bradykinesia, tremor, rest tremor, appendicular, left motor score, right motor score, asymmetry, total updrs 3 and total updrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Motor_Assessments/MDS_UPDRS_Part_III.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='BL']\n",
    "x=x.iloc[:,np.r_[2,11:44]]\n",
    "x['Axial_Sub_1']=x.loc[:,['NP3SPCH','NP3FACXP','NP3RIGN',\n",
    "                     'NP3RISNG','NP3GAIT','NP3FRZGT',\n",
    "                     'NP3PSTBL','NP3POSTR','NP3BRADY','NP3RTALJ']].sum(axis=1,skipna=False)\n",
    "x['Axial_Sub_2']=x.loc[:,['NP3RISNG','NP3GAIT','NP3FRZGT',\n",
    "                     'NP3PSTBL','NP3POSTR','NP3BRADY']].sum(axis=1,skipna=False)\n",
    "x['Limb_Rig_Sub']=x.loc[:,['NP3RIGRU','NP3RIGLU','PN3RIGRL','NP3RIGLL']].sum(axis=1,skipna=False)\n",
    "x['Limb_Brady_Sub']=x.loc[:,['NP3FTAPR','NP3FTAPL','NP3HMOVR','NP3HMOVL',\n",
    "                             'NP3PRSPR','NP3PRSPL','NP3TTAPR','NP3TTAPL',\n",
    "                            'NP3LGAGR','NP3LGAGL']].sum(axis=1,skipna=False)\n",
    "x['Tremor_Sub']=x.loc[:,['NP3PTRMR','NP3PTRML','NP3KTRMR','NP3KTRML','NP3RTARU',\n",
    "                          'NP3RTALU','NP3RTARL','NP3RTALL','NP3RTCON']].sum(axis=1,skipna=False)\n",
    "x['Rest_Tremor_Sub']=x.loc[:,['NP3RTARU','NP3RTALU','NP3RTARL','NP3RTALL','NP3RTCON']].sum(axis=1,skipna=False)\n",
    "x['Append_Sub']=x.loc[:,['Limb_Rig_Sub','Limb_Brady_Sub','Tremor_Sub']].sum(axis=1,skipna=False)\n",
    "x['Left_Motor_Score']=x.loc[:,['NP3RIGLU','NP3RIGLL','NP3FTAPL','NP3HMOVL',\n",
    "                                 'NP3PRSPL','NP3TTAPL','NP3LGAGL','NP3PTRML',\n",
    "                                 'NP3KTRML','NP3RTALU','NP3RTALL']].sum(axis=1,skipna=False)\n",
    "x['Right_Motor_Score']=x.loc[:,['NP3RIGRU','PN3RIGRL','NP3FTAPR','NP3HMOVR',\n",
    "                                  'NP3PRSPR','NP3TTAPR','NP3LGAGR','NP3PTRMR',\n",
    "                                  'NP3KTRMR','NP3RTARU','NP3RTARL']].sum(axis=1,skipna=False)\n",
    "x['Diff']=x['Left_Motor_Score']-x['Right_Motor_Score']\n",
    "x['Asymmetry']=np.abs(x['Diff'])\n",
    "\n",
    "x['Total_UPDRS3']=x.loc[:,'NP3SPCH':'NP3RTCON'].sum(axis=1,skipna=False)\n",
    "\n",
    "x=x.loc[:,['PATNO','Axial_Sub_1','Axial_Sub_2','Limb_Rig_Sub','Limb_Brady_Sub','Tremor_Sub','Rest_Tremor_Sub',\n",
    "          'Append_Sub','Left_Motor_Score','Right_Motor_Score','Diff','Asymmetry','Total_UPDRS3']]\n",
    "\n",
    "BL_Feat=BL_Feat.merge(x,on='PATNO',how='left')\n",
    "BL_Feat['Total_UPDRS']=BL_Feat['Total_UPDRS1']+BL_Feat['Total_UPDRS2']+BL_Feat['Total_UPDRS3']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Schwab and England ADL\n",
    "### in the file is present the final total score, I used it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Motor_Assessments/Modified_Schwab_+_England_ADL.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='BL']\n",
    "x=x.loc[:,['PATNO','MSEADLG']]\n",
    "BL_Feat=BL_Feat.merge(x,on='PATNO',how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASE Household activity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO DATA AT BL OR SCREENING\n",
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Motor_Assessments/PASE_-_Household_Activity.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='BL']\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non- Motor Symptoms\n",
    "## Benton Judgment of line orientation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Non-Motor-Asses/Neurophysiological/Benton_Judgment_of_Line_Orientation.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='BL']\n",
    "x=x.loc[:,['PATNO','DVS_JLO_MSSAE']]\n",
    "BL_Feat=BL_Feat.merge(x,on='PATNO',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cognitive categorization\n",
    "### at BL there are data for 106 patients instead of 423, 75% of data missing. useless file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#106 on 423 = 75% of data missing on cognitive categorization\n",
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Non-Motor-Asses/Cognition/Cognitive_Categorization.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='BL']\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epworth Sleepiness Scale\n",
    "### In the original paper they suggest to made the sum of the 8 questions. I have used the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Non-Motor-Asses/Sleep_Disorder/Epworth_Sleepiness_Scale.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='BL']\n",
    "x['Epworth_SUM']=x.loc[:,'ESS1':'ESS8'].sum(axis=1,skipna=False)\n",
    "x=x.loc[:,['PATNO','Epworth_SUM']]\n",
    "BL_Feat=BL_Feat.merge(x,on='PATNO',how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geriatric depression scale short\n",
    "### In the original paper they suggest the sum of the question outcome. I have used it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Non-Motor-Asses/NeuroBehavioral/Geriatric_Depression_Scale__Short_.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='BL']\n",
    "x=x.iloc[:,np.r_[2,6:21,]]\n",
    "no=['GDSSATIS', 'GDSGSPIR', 'GDSHAPPY', 'GDSALIVE', 'GDSENRGY']\n",
    "x['GDS_SUM']=np.zeros(len(x))\n",
    "for i in no:\n",
    "    x[i]=1-x[i]\n",
    "x['GDS_SUM']=x.loc[:,'GDSSATIS':'GDSBETER'].sum(axis=1,skipna=False)\n",
    "x=x.loc[:,['PATNO','GDS_SUM']]\n",
    "BL_Feat=BL_Feat.merge(x,on='PATNO',how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hopkins verbal learning test\n",
    "### Derived-Total Recall T-Score, Derived-Delayed Recall T-Score, Derived-Retention T-Score, Derived-Recog. Discrim. Index T-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Non-Motor-Asses/Neurophysiological/Hopkins_Verbal_Learning_Test.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='BL']\n",
    "x=x.iloc[:,np.r_[2,16]]\n",
    "BL_Feat=BL_Feat.merge(x,on='PATNO',how='left')\n",
    "BL_Feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter number sequencing PD\n",
    "### In the file there is the total score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Non-Motor-Asses/Neurophysiological/Letter_-_Number_Sequencing__PD_.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='BL']\n",
    "x=x.loc[:,['PATNO','DVS_LNS']]\n",
    "BL_Feat=BL_Feat.merge(x,on='PATNO',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montreal cognitive assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Non-Motor-Asses/Neurophysiological/Montreal_Cognitive_Assessment__MoCA_.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='SC']\n",
    "x=x.loc[:,['PATNO','MCATOT']]\n",
    "BL_Feat=BL_Feat.merge(x,on='PATNO',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionnaire for impulsive compulsive disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Non-Motor-Asses/NeuroBehavioral/QUIP_Current_Short.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='BL']\n",
    "x=x.iloc[:,np.r_[2,7:20]]\n",
    "x['QUIP_Sum']=x.loc[:,'TMGAMBLE':'CNTRLDSM'].sum(axis=1,skipna=False)\n",
    "x['QUIP_Buying']=x.loc[:,['TMBUY','CNTRLBUY']].sum(axis=1,skipna=False)\n",
    "x['QUIP_Eating']=x.loc[:,['TMEAT','CNTRLEAT']].sum(axis=1,skipna=False)\n",
    "x['QUIP_Gamble']=x.loc[:,['TMGAMBLE','CNTRLGMB']].sum(axis=1,skipna=False)\n",
    "x['QUIP_Hobbies']=x.loc[:,['TMTORACT']].sum(axis=1,skipna=False)\n",
    "x['QUIP_Punding']=x.loc[:,['TMTMTACT']].sum(axis=1,skipna=False)\n",
    "x['QUIP_Sex']=x.loc[:,['TMSEX','CNTRLSEX']].sum(axis=1,skipna=False)\n",
    "x['QUIP_Driving']=x.loc[:,['TMTRWD']].sum(axis=1,skipna=False)\n",
    "                          \n",
    "x=x.loc[:,['PATNO','QUIP_Sum']]\n",
    "BL_Feat=BL_Feat.merge(x,on='PATNO',how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REM Sleep disorder questionnaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Non-Motor-Asses/Sleep_Disorder/REM_Sleep_Disorder_Questionnaire.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='BL']\n",
    "x=x.iloc[:,np.r_[2,7:28]]\n",
    "x['REM_Sum']=x.loc[:,'DRMVIVID':'CNSOTH'].sum(axis=1,skipna=False)\n",
    "x=x.loc[:,['PATNO','REM_Sum']]\n",
    "BL_Feat=BL_Feat.merge(x,on='PATNO',how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCOPA-AUT: Scale for outcomes in PD for autonomic symptoms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Non-Motor-Asses/Autonomic/SCOPA-AUT.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='BL']\n",
    "for i in x.loc[:,'SCAU1':'SCAU21'].columns:\n",
    "    x[i]=x[i].replace(9,3)\n",
    "for i in x.loc[:,'SCAU22':'SCAU25'].columns: \n",
    "    x[i]=x[i].replace(9,0)\n",
    "x['SCOPA_TOT']=x.loc[:,'SCAU1':'SCAU25'].sum(axis=1)\n",
    "x['SCOPA_Gastro']=x.loc[:,'SCAU1':'SCAU7'].sum(axis=1)\n",
    "x['SCOPA_Urinary']=x.loc[:,'SCAU8':'SCAU13'].sum(axis=1)\n",
    "x['SCOPA_Cardio']=x.loc[:,'SCAU14':'SCAU16'].sum(axis=1)\n",
    "x['SCOPA_Pupillomotor']=x.loc[:,'SCAU19']\n",
    "x['SCOPA_Thermoreg']=x.loc[:,['SCAU17','SCAU18','SCAU20','SCAU21']].sum(axis=1)\n",
    "x['SCOPA_Sex']=x.loc[:,'SCAU22':'SCAU25'].sum(axis=1)\n",
    "x=x.loc[:,['PATNO','SCOPA_TOT']]\n",
    "BL_Feat=BL_Feat.merge(x,on='PATNO',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical fluency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SIETE FAKE NON CI SONO i DATI ALLA BASELINE\n",
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Non-Motor-Asses/Neurophysiological/Lexical_Fluency.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='SC']\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAI, Stait trait anxiety inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Non-Motor-Asses/NeuroBehavioral/State-Trait_Anxiety_Inventory.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='BL']\n",
    "x=x.iloc[:,np.r_[2,6:46]]\n",
    "rev=[1, 2, 5, 8, 10, 11, 15, 16, 19, 20, 21, 23, 26, 27, 30, 33, 34, 36, 39]\n",
    "for (j,i) in zip(x.iloc[:,1:].columns,range(1,41)):\n",
    "    if i in rev:\n",
    "        x[j]=5-x[j]\n",
    "x['STAI_Sum_S']=x.loc[:,'STAIAD1':'STAIAD20'].sum(axis=1,skipna=False)\n",
    "x['STAI_Sum_T']=x.loc[:,'STAIAD21':'STAIAD40'].sum(axis=1,skipna=False)\n",
    "x['STAI_Sum']=x['STAI_Sum_S']+x['STAI_Sum_T']\n",
    "x=x.loc[:,['PATNO','STAI_Sum']]\n",
    "BL_Feat=BL_Feat.merge(x,on='PATNO',how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbol digit Modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Non-Motor-Asses/Neurophysiological/Symbol_Digit_Modalities.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='BL']\n",
    "x=x.loc[:,['PATNO','DVT_SDM']]\n",
    "BL_Feat=BL_Feat.merge(x,on='PATNO',how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## University of Pennysylvania smell id test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('Desktop/Thesis/PPMI_Dataset/Non-Motor-Asses/Olfactory/University_of_Pennsylvania_Smell_ID_Test.csv')\n",
    "x=x[x['PATNO'].isin(PD_patlist)]\n",
    "x=x[x['EVENT_ID']=='BL']\n",
    "x=x.loc[:,['PATNO','UPSITBK1','UPSITBK2','UPSITBK3','UPSITBK4']]\n",
    "x['UPSIT_Sum']=x.loc[:,'UPSITBK1':'UPSITBK4'].sum(axis=1,skipna=False)\n",
    "x=x.loc[:,['PATNO','UPSIT_Sum']]\n",
    "BL_Feat=BL_Feat.merge(x,on='PATNO',how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var=['ageonset','agediag','DOMSIDE']\n",
    "for i in var:\n",
    "    BL_Feat[i]=BL_Feat[i].astype('float64')\n",
    "print(BL_Feat.columns)\n",
    "BL_Feat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BL_Feat=BL_Feat.merge(prog.loc[:,['PATNO','Diff_BL_4yr']].dropna(),how='left',on='PATNO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BL_Feat=BL_Feat[np.isnan(BL_Feat['Diff_BL_4yr'])==False]\n",
    "BL_Feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BL_Feat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BL_Feat=BL_Feat.drop(columns=['Diff','Left_Motor_Score','Right_Motor_Score','Asymmetry',\n",
    "                              'Total_UPDRS1','Total_UPDRS2','Total_UPDRS3','Total_UPDRS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BL_Feat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early=BL_Feat[BL_Feat['PATNO'].isin(group_early)].copy()\n",
    "others=BL_Feat[BL_Feat['PATNO'].isin(group_others)].copy()\n",
    "early['Class']=1\n",
    "others['Class']=0\n",
    "whole=pd.concat([early,others])\n",
    "whole=whole.sort_values(by=['PATNO']).reset_index().drop(columns=['index','Diff_BL_4yr'])\n",
    "whole\n",
    "#whole.to_csv('BL_features_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers\n",
    "## No medical reason to exclude few of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "X=whole.iloc[:,1:-1]\n",
    "Y=whole.iloc[:,-1]\n",
    "import seaborn as sns\n",
    "var=scale(X)\n",
    "warnings.filterwarnings('ignore')\n",
    "for j in range(np.shape(var)[1]):\n",
    "    plt.figure()\n",
    "    plt.title(X.iloc[:,j].name)\n",
    "    plt.boxplot(var[:,j][~np.isnan(var[:,j])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop row with missing values, Standardization, Division in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Drop row with missing values\n",
    "ind=X[X.isna().any(axis=1)].index\n",
    "X=X.drop(index=ind).reset_index().drop(columns='index')\n",
    "Y=Y.drop(index=ind).reset_index().drop(columns='index')\n",
    "\n",
    "\n",
    "    \n",
    "#Standardization\n",
    "\n",
    "for col in X.columns:\n",
    "    if col=='gen' or col=='DOMSIDE':\n",
    "        continue\n",
    "    else:\n",
    "        X[col]=(X[col]-X[col].mean())/X[col].std()\n",
    "        \n",
    "#Correlation analysis\n",
    "corr=X.corr().abs()\n",
    "sns.heatmap(X.corr())\n",
    "feat=list(X.columns)\n",
    "drop=[]\n",
    "for i in range(len(corr)):\n",
    "    for j in range(i+1,len(corr)):\n",
    "            if np.abs(corr.iloc[i,j]) >0.8:\n",
    "                print(feat[i],feat[j],corr.iloc[i,j])\n",
    "                drop.append(feat[i])\n",
    "                drop.append(feat[j])\n",
    "drop=list(dict.fromkeys(drop))\n",
    "keep=['age','Axial_Sub_1','Append_Sub','Tremor_Sub']\n",
    "drop=list(set(drop)-set(keep))\n",
    "X=X.drop(columns=drop)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[X['gen']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['DVT_SDM'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Division in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1,stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('X_train:',len(X_train))\n",
    "print('X_test:',len(X_test))\n",
    "print('y_train early:',len(y_train[y_train['Class']==1]))\n",
    "print('y_train others:',len(y_train[y_train['Class']==0]))\n",
    "print('y_test early:',len(y_test[y_test['Class']==1]))\n",
    "print('y_test others:',len(y_test[y_test['Class']==0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models without feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Model without feature selection\n",
    "from sklearn.metrics import f1_score,confusion_matrix,precision_score,balanced_accuracy_score,recall_score,accuracy_score,roc_auc_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "models=[RandomForestClassifier(n_estimators=5,n_jobs=-1,class_weight='balanced_subsample',bootstrap=True,random_state=0),\n",
    "       GaussianNB(),LogisticRegression(class_weight='balanced'),DummyClassifier(strategy='most_frequent'),\n",
    "        SGDClassifier('log',class_weight='balanced',learning_rate='adaptive',eta0=0.001), DecisionTreeClassifier(class_weight='balanced',),\n",
    "        AdaBoostClassifier(n_estimators=30,random_state=0),GradientBoostingClassifier(n_estimators=10,random_state=0)]\n",
    "model_names=['RF','GNB','LOG_Reg','Dummy','SGDClass','DecTree','ADA','GBC']\n",
    "\n",
    "for model,model_name in zip(models,model_names):\n",
    "    print(model_name)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred=model.predict(X_test)\n",
    "    print('F1:',f1_score(y_test,y_pred))\n",
    "    print('Precision:',precision_score(y_test,y_pred))\n",
    "    print('Recall:',recall_score(y_test,y_pred))\n",
    "    print('Confusion matrix:\\n',confusion_matrix(y_test,y_pred))\n",
    "    print('Accuracy:',balanced_accuracy_score(y_test,y_pred))\n",
    "    try:\n",
    "        print('Roc AUC:',roc_auc_score(y_test,y_pred))\n",
    "    except:\n",
    "        print('ROC AUC: Nan')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE on original training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTENC,BorderlineSMOTE,ADASYN\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "models=[RandomForestClassifier(n_estimators=10,n_jobs=-1,bootstrap=True),\n",
    "       GaussianNB(),LogisticRegression(),\n",
    "        SGDClassifier('log',learning_rate='adaptive',eta0=0.001), DecisionTreeClassifier(),\n",
    "        AdaBoostClassifier()]\n",
    "model_names=['RF','GNB','LOG_Reg','SGDClass','DecTree','ADA']\n",
    "for model,model_name in zip(models,model_names):\n",
    "    print(model_name)\n",
    "    print(Counter(y_train['Class']))\n",
    "    oversample = SMOTENC(categorical_features=[1,2,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,21,\n",
    "                                              22,23,24,25,26,27,28,29,30,31],k_neighbors=1)\n",
    "    X_train_s, y_train_s = oversample.fit_sample(X_train, y_train)\n",
    "    print(Counter(y_train_s['Class']))\n",
    "    model.fit(X_train_s,y_train_s)\n",
    "    y_pred=model.predict(X_test)\n",
    "    y_prob=model.predict_proba(X_test)\n",
    "    print('F1:',f1_score(y_test,y_pred))\n",
    "    print('Precision:',precision_score(y_test,y_pred))\n",
    "    print('Recall:',recall_score(y_test,y_pred))\n",
    "    print('Confusion matrix:\\n',confusion_matrix(y_test,y_pred))\n",
    "    print('Accuracy:',balanced_accuracy_score(y_test,y_pred))\n",
    "    try:\n",
    "        print('Roc AUC:',roc_auc_score(y_test,y_prob))\n",
    "    except:\n",
    "        print('ROC AUC: Nan')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE + UNDERSAMPLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "models=[RandomForestClassifier(n_estimators=10,n_jobs=-1,bootstrap=True),\n",
    "       GaussianNB(),LogisticRegression(),\n",
    "        SGDClassifier('log',learning_rate='adaptive',eta0=0.001), DecisionTreeClassifier(),\n",
    "        AdaBoostClassifier()]\n",
    "model_names=['RF','GNB','LOG_Reg','SGDClass','DecTree','ADA']\n",
    "for model,model_name in zip(models,model_names):\n",
    "    print(model_name)\n",
    "    print(Counter(y_train['Class']))\n",
    "    oversample =SMOTENC(categorical_features=[1,2,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,21,\n",
    "                                            22,23,24,25,26,27,28,29,30,31],sampling_strategy=1,k_neighbors=1)\n",
    "    undersample=RandomUnderSampler(0.5)\n",
    "    \n",
    "    X_train_s,y_train_s= undersample.fit_resample(X_train,y_train)\n",
    "    X_train_s, y_train_s = oversample.fit_resample(X_train_s, y_train_s)\n",
    "    \n",
    "    print(Counter(y_train_s['Class']))\n",
    "    model.fit(X_train_s,y_train_s)\n",
    "    y_pred=model.predict(X_test)\n",
    "    print('F1:',f1_score(y_test,y_pred))\n",
    "    print('Precision:',precision_score(y_test,y_pred))\n",
    "    print('Recall:',recall_score(y_test,y_pred))\n",
    "    print('Confusion matrix:\\n',confusion_matrix(y_test,y_pred))\n",
    "    print('Accuracy:',balanced_accuracy_score(y_test,y_pred))\n",
    "    try:\n",
    "        print('Roc AUC:',roc_auc_score(y_test,y_pred))\n",
    "    except:\n",
    "        print('ROC AUC: Nan')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature selection with K-Fold cross val on training set.\n",
    "# test performances on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "models=[RandomForestClassifier(n_estimators=10,n_jobs=-1,bootstrap=True,class_weight='balanced_subsample'),\n",
    "       GaussianNB(),LogisticRegression(class_weight='balanced'),DummyClassifier(strategy='most_frequent'),\n",
    "        SGDClassifier('log',learning_rate='adaptive',eta0=0.01,class_weight='balanced'), DecisionTreeClassifier(class_weight='balanced'),\n",
    "        AdaBoostClassifier(n_estimators=10),GradientBoostingClassifier(n_estimators=10)]\n",
    "\n",
    "model_names=['RF','GNB','LOG_Reg','Dummy','SGDClass','DecTree','ADA','GBC']\n",
    "n_feat=10\n",
    "for model,model_name in zip(models,model_names):\n",
    "    feat={key:0 for key in X_train.columns}\n",
    "    print(model_name)\n",
    "    skf=StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        X_train_K, X_test_K = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "        y_train_K, y_test_K = y_train.iloc[train_index,:], y_train.iloc[test_index,:]\n",
    "        selector = SelectKBest(mutual_info_classif, k=n_feat)\n",
    "        selector.fit(X_train_K, y_train_K)\n",
    "        support = selector.get_support()\n",
    "        for j in X_train_K.loc[:,support].columns.to_list():\n",
    "            feat[j]+=1\n",
    "    feat={k: v for k, v in sorted(feat.items(), key=lambda feat: feat[1],reverse=True)}\n",
    "    support=list(feat.keys())[:n_feat+1]\n",
    "    X_train_feat=X_train.loc[:,support]\n",
    "    X_test_feat=X_test.loc[:,support]\n",
    "    model.fit(X_train_feat,y_train)\n",
    "    y_pred=model.predict(X_test_feat)\n",
    "    y_prob=model.predict_proba(X_test_feat)\n",
    "    print('F1:',f1_score(y_test,y_pred))\n",
    "    print('Precision:',precision_score(y_test,y_pred))\n",
    "    print('Recall:',recall_score(y_test,y_pred))\n",
    "    print('Confusion matrix:\\n',confusion_matrix(y_test,y_pred))\n",
    "    print('Accuracy:',balanced_accuracy_score(y_test,y_pred))\n",
    "    try:\n",
    "        print('Roc AUC:',roc_auc_score(y_test,y_prob))\n",
    "    except:\n",
    "        print('ROC AUC: Nan')\n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold,RepeatedKFold\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "models=[RandomForestClassifier(n_estimators=10,n_jobs=-1,bootstrap=True,class_weight='balanced_subsample'),\n",
    "       GaussianNB(),LogisticRegression(class_weight='balanced'),DummyClassifier(strategy='most_frequent'),\n",
    "        SGDClassifier('log',learning_rate='adaptive',eta0=0.01,class_weight='balanced'), DecisionTreeClassifier(class_weight='balanced'),\n",
    "        AdaBoostClassifier(n_estimators=10),GradientBoostingClassifier(n_estimators=10)]\n",
    "\n",
    "model_names=['RF','GNB','LOG_Reg','Dummy','SGDClass','DecTree','ADA','GBC']\n",
    "for model,model_name in zip(models,model_names):\n",
    "    acc,f1,p,r,accb,roc = [],[],[],[],[],[]\n",
    "    print(model_name)\n",
    "    skf=StratifiedKFold(n_splits=7,random_state=0,shuffle=True)\n",
    "    for train_index, test_index in skf.split(X, Y):\n",
    "        X_train_K, X_test_K = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train_K, y_test_K = Y.iloc[train_index,:], Y.iloc[test_index,:]\n",
    "        # select best features on training \n",
    "        #print(list(X_train_K.columns))\n",
    "    \n",
    "        model = model.fit(X_train_K,y_train_K.values.ravel())\n",
    "        y_pred = model.predict(X_test_K)\n",
    "        y_prob=model.predict_proba(X_test_K)[:,1]\n",
    "        acc.append(accuracy_score(y_test_K,y_pred))\n",
    "        f1.append(f1_score(y_test_K,y_pred))\n",
    "        p.append(precision_score(y_test_K,y_pred))\n",
    "        r.append(recall_score(y_test_K,y_pred))\n",
    "        accb.append(balanced_accuracy_score(y_test_K,y_pred))\n",
    "        try:\n",
    "            roc.append(roc_auc_score(y_test_K,y_prob))\n",
    "        except:\n",
    "            roc.append(np.nan)\n",
    "        \n",
    "\n",
    "    print('Acc mean',np.mean(acc))    \n",
    "    print('F1 mean',np.mean(f1))\n",
    "    print('Precision mean',np.mean(p))\n",
    "    print('Recall mean',np.mean(r))\n",
    "    print('Balance Acc mean',np.mean(accb))\n",
    "    try:\n",
    "        print('Roc AUC:',np.mean(roc))\n",
    "    except:\n",
    "        print('ROC AUC: Nan')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross validation + smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models=[RandomForestClassifier(n_estimators=10,n_jobs=-1,bootstrap=True,class_weight='balanced_subsample'),\n",
    "       GaussianNB(),LogisticRegression(class_weight='balanced'),DummyClassifier(strategy='most_frequent'),\n",
    "        SGDClassifier('log',learning_rate='adaptive',eta0=0.01,class_weight='balanced'), DecisionTreeClassifier(class_weight='balanced'),\n",
    "        AdaBoostClassifier(n_estimators=10),GradientBoostingClassifier(n_estimators=10)]\n",
    "\n",
    "model_names=['RF','GNB','LOG_Reg','Dummy','SGDClass','DecTree','ADA','GBC']\n",
    "for model,model_name in zip(models,model_names):\n",
    "    acc,f1,p,r,accb,roc = [],[],[],[],[],[]\n",
    "    print(model_name)\n",
    "    skf=StratifiedKFold(n_splits=5,random_state=1,shuffle=True)\n",
    "    for train_index, test_index in skf.split(X, Y):\n",
    "        X_train_K, X_test_K = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train_K, y_test_K = Y.iloc[train_index,:], Y.iloc[test_index,:]\n",
    "                \n",
    "\n",
    "\n",
    "        oversample =SMOTENC(categorical_features=[1,2,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,21,\n",
    "                                            22,23,24,25,26,27,28,29,30],sampling_strategy=1,k_neighbors=1)\n",
    "        undersample=RandomUnderSampler(0.5)\n",
    "        X_train_K, y_train_K = undersample.fit_resample(X_train_K, y_train_K)\n",
    "        X_train_K, y_train_K = oversample.fit_resample(X_train_K, y_train_K)\n",
    "        model = model.fit(X_train_K,y_train_K.values.ravel())\n",
    "        y_pred = model.predict(X_test_K)\n",
    "        y_prob=model.predict_proba(X_test_K)[:,1]\n",
    "        acc.append(accuracy_score(y_test_K,y_pred))\n",
    "        f1.append(f1_score(y_test_K,y_pred))\n",
    "        p.append(precision_score(y_test_K,y_pred))\n",
    "        r.append(recall_score(y_test_K,y_pred))\n",
    "        accb.append(balanced_accuracy_score(y_test_K,y_pred))\n",
    "        try:\n",
    "            roc.append(roc_auc_score(y_test_K,y_prob))\n",
    "        except:\n",
    "            roc.append(np.nan)\n",
    "\n",
    "\n",
    "    print('Acc mean',np.mean(acc))    \n",
    "    print('F1 mean',np.mean(f1))\n",
    "    print('Precision mean',np.mean(p))\n",
    "    print('Recall mean',np.mean(r))\n",
    "    print('Balance Acc mean',np.mean(accb))\n",
    "    try:\n",
    "        print('Roc AUC:',np.mean(roc))\n",
    "    except:\n",
    "        print('ROC AUC: Nan')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold +SMOTE+ feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold,RepeatedKFold,LeaveOneOut\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.stats import mannwhitneyu\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE,SMOTENC\n",
    "#model=DecisionTreeClassifier(class_weight='balanced')\n",
    "#model=GradientBoostingClassifier(n_estimators=100)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=1,shuffle=True)\n",
    "model=LogisticRegression(class_weight='balanced')\n",
    "#model=GaussianNB()\n",
    "#model=RandomForestClassifier(n_estimators=100,n_jobs=-1,bootstrap=True,class_weight='balanced_subsample')\n",
    "#model=SGDClassifier('log',learning_rate='adaptive',eta0=0.005,class_weight='balanced')\n",
    "n_feat=[1,2,3,4,5,6,7,8,9,10]\n",
    "#X=X.drop(columns=['Diff_BL_1yr'])\n",
    "for f in n_feat:\n",
    "    print('\\nN Feat:',f)\n",
    "    \n",
    "    skf=StratifiedKFold(n_splits=10,random_state=1,shuffle=False)\n",
    "    skf1=StratifiedKFold(n_splits=10,random_state=1,shuffle=False)\n",
    "    loo=LeaveOneOut()\n",
    "    acc,f1,p,r,accb,roc,brier,mcc = [],[],[],[],[],[],[],[]\n",
    "    tot_feat={key:0 for key in X.columns}    \n",
    "    for train_index, test_index in skf1.split(X, Y):\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train, y_test = Y.iloc[train_index,:], Y.iloc[test_index,:]\n",
    "        y_p=[]\n",
    "        y_t=[]\n",
    "        feat={key:0 for key in X.columns}\n",
    "        for train_index, test_index in skf.split(X_train, y_train):\n",
    "            X_train_K, X_test_K = X_train.iloc[train_index,:], X_train.iloc[test_index,:]\n",
    "            y_train_K, y_test_K = y_train.iloc[train_index,:], y_train.iloc[test_index,:]\n",
    "\n",
    "\n",
    "            ###VARIABLE RANKING\n",
    "            rank={key:0 for key in X_train.columns}\n",
    "            for j in X_train.columns:\n",
    "                rank[j]=mannwhitneyu(X_train_K[y_train_K['Class']==0][j],X_train_K[y_train_K['Class']==1][j])[1]\n",
    "            rank={k: v for k, v in sorted(rank.items(), key=lambda item: item[1],reverse=False)}  \n",
    "            support=list(rank.keys())[0:f]\n",
    "\n",
    "            \n",
    "\n",
    "            ###INCREMENT FEATURE COUNTER\n",
    "            for j in X_train_K.loc[:,support].columns.to_list():\n",
    "                feat[j]+=1\n",
    "                tot_feat[j]+=1\n",
    "                \n",
    "\n",
    "            ###SELECT BEST N FEAT\n",
    "            X_train_K = X_train_K.loc[:,support]\n",
    "\n",
    "\n",
    "            ###SMOTE\n",
    "            under=RandomUnderSampler(0.5,random_state=0)\n",
    "            oversample=SMOTENC(sampling_strategy=1,categorical_features=[i for i in range(f)],k_neighbors=5,random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print('TEST')\n",
    "        tot_feat={k: v for k, v in sorted(tot_feat.items(), key=lambda item: item[1],reverse=True)}\n",
    "        feat={k: v for k, v in sorted(feat.items(), key=lambda item: item[1],reverse=True)}\n",
    "        print(list(feat.keys())[0:f])\n",
    "\n",
    "        X_train_ov=X_train.loc[:,list(feat.keys())[0:f]]\n",
    "        X_test_ov=X_test.loc[:,list(feat.keys())[0:f]]\n",
    "        \n",
    "        X_train_ov,y_train_ov=under.fit_resample(X_train_ov, y_train)\n",
    "        \n",
    "        X_train_ov['temp']=0\n",
    "        X_train_ov, y_train_ov = oversample.fit_resample(X_train_ov, y_train_ov)\n",
    "        X_train_ov=X_train_ov.drop(columns=['temp'])\n",
    "        \n",
    "        model=model.fit(X_train_ov,y_train_ov)\n",
    "        \n",
    "        y_pred=model.predict(X_test_ov)\n",
    "        y_prob=model.predict_proba(X_test_ov)[:,1]\n",
    "\n",
    "        acc.append(accuracy_score(y_test,y_pred))\n",
    "        f1.append(f1_score(y_test,y_pred))\n",
    "        p.append(precision_score(y_test,y_pred))\n",
    "        r.append(recall_score(y_test,y_pred))\n",
    "        accb.append(balanced_accuracy_score(y_test,y_pred))\n",
    "        roc.append(roc_auc_score(y_test,y_prob))\n",
    "        brier.append(brier_score_loss(y_test,y_pred))\n",
    "        mcc.append(matthews_corrcoef(y_test,y_pred))\n",
    "        \n",
    "\n",
    "    print('Acc mean',np.mean(acc))    \n",
    "    print('F1 mean',np.mean(f1))\n",
    "    print('Precision mean',np.mean(p))\n",
    "    print('Recall mean',np.mean(r))\n",
    "    print('Balance Acc mean',np.mean(accb))\n",
    "    try:\n",
    "        print('Roc AUC:',np.mean(roc))\n",
    "    except:\n",
    "        print('ROC AUC: Nan')\n",
    "    print('Brier',np.mean(brier))\n",
    "    print('MCC',np.mean(mcc))\n",
    "    \n",
    "    tot_feat={k: v for k, v in sorted(tot_feat.items(), key=lambda item: item[1],reverse=True)}\n",
    "    print(tot_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures as pol\n",
    "from sklearn.pipeline import make_pipeline\n",
    "model_pol=make_pipeline(pol(10),LinearRegression())\n",
    "lab=prog.loc[:,['PATNO','Diff_BL_4yr']].dropna()\n",
    "dat=whole.merge(lab,how='left',on='PATNO').dropna()\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat=dat.drop(columns=['PATNO','Diff_BL_1yr','Class'])\n",
    "X_reg=dat.iloc[:,:-1]\n",
    "Y_reg=dat.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_reg.columns:\n",
    "    if col=='gen' or col=='DOMSIDE':\n",
    "        continue\n",
    "    else:\n",
    "        X_reg[col]=(X_reg[col]-X_reg[col].mean())/X_reg[col].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \"Desktop/ML_Chap.html\"\n",
    "\n",
    "with open(FILE, 'r') as html_file:\n",
    "    content = html_file.read()\n",
    "\n",
    "# Get rid off prompts and source code\n",
    "content = content.replace(\"div.input_area {\",\"div.input_area {\\n\\tdisplay: none;\")    \n",
    "content = content.replace(\".prompt {\",\".prompt {\\n\\tdisplay: none;\")\n",
    "\n",
    "f = open(FILE, 'w')\n",
    "f.write(content)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}